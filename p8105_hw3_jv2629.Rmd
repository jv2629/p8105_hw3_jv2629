---
title: "p8105_hw3_jv2629"
author: "Jaclyn Verity - jv2629"
date: "October 10, 2018"
output: github_document
---

#Setup the the document for success!
```{r setup}
knitr::opts_chunk$set(echo = TRUE)
  fig.width = 10
  fig.asp = .6
  out.width = "90%"

library(tidyverse)
library(ggridges)
library(knitr)
  
theme_set(theme_bw() + theme(legend.position = "bottom"))
```

#Problem 1
```{r}
library(p8105.datasets)

beh_risk = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
 rename(state = locationabbr, county = locationdesc)

beh_risk = beh_risk %>% 
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"))) %>% 
  arrange(desc(response))
```

**In 2002, which states were observed at 7 locations?**
```{r first question}
beh_risk %>% 
  filter(year == 2002) %>%
  group_by(state) %>% 
  distinct(state, county) %>%  
  summarise(county = n())
```

In 2002, North Carolina, Florida, and Connecticut were observed at 7 locations. 


**Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.**
```{r}
spag_plot = beh_risk %>% 
  group_by(year, state) %>% 
  distinct(county, .keep_all = TRUE) %>%  
  summarise(county = n()) 

ggplot(data = spag_plot, aes(x = year , y = county, group = state, color = state)) + geom_line()

```


**Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.**
```{r}
ny_excellent = beh_risk %>% 
  filter(year %in% c(2002, 2006, 2010)) %>%
  filter(state == "NY") %>% 
  filter(response == "Excellent") %>% 
  group_by(year) %>% 
  summarise(
    mean(data_value), sd(data_value))

kable(ny_excellent)
```

**For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.**
```{r}
resp5_plot = beh_risk %>% 
  select(year, state, county, response, data_value) %>% 
  group_by(year, state, response) %>% 
  summarise(
    avg_prop = mean(data_value)) 

ggplot(resp5_plot, aes(x = year, y = avg_prop, color = state)) + 
  geom_line(alpha = .5) +
  facet_grid(. ~ response)
```

#Problem 2

**Describe the instacart data set.**
```{r}
instacart
```

The instacart data set appears to be data about online orders for groceries from a grocery chain. The data set is 15 columns by 1,384,617 rows. Each row represents a specific item ordered by a customer. The columns represent different information about each item ordered. Many of the items belong to the same order/cart/person as they have the same data values for order_id, user_id, order_dow, order_hour_of_the_day, and days_since_prior_order. Each item has a product_id, a text name, what aisle it is found on, and in what department the product is registered under. For example, as the raw data stands now, row 6 contains product_id 13176, A Bag of Organic Bananas, located in aisle 24 - fresh fruits, department 4 - produce. The data set also contains data on the user's habits/process in ordering: how often the user orders this item, how many orders they have placed, the order in which the item was added to the cart, and the time of week/day the order was placed. 

**How many aisles are there, and which aisles are the most items ordered from?**
```{r}
instacart %>% 
  distinct(aisle_id) %>% 
  arrange(desc(aisle_id))

instacart %>% 
  count(aisle_id) %>% 
  arrange(desc(n))

instacart %>% 
  filter(aisle_id %in% c(83, 24, 123))
```

There are 134 aisles. The aisles with the most orders are aisles 83 - fresh vegetables, 24 - fresh fruits, and 123 - packaged vegetables fruits with 150,609, 150,473, and 78,493 items ordered, respectively. 

**Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.**
```{r}
aisle_plot = instacart %>%
  group_by(aisle) %>% 
  summarise(n = n()) %>% 
  arrange(n)

ggplot(aisle_plot, aes(x = aisle, y = n)) + 
  geom_point()+
  labs(
    title = "Number of Items per Aisle",
    x = "Aisle",
    y = "Number of Items Ordered",
    caption = "Data from the p8105 package, data set instacart"
  ) + scale_x_discrete(labels = abbreviate)
```


**Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.**
```{r}
popular = instacart %>% 
  select(aisle, product_id, product_name) %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  summarise(most_pop = max(product_name), na.rm = TRUE)

kable(popular)
```

**Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).**
```{r}
mean_hour = instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarise(mean_time = mean(order_hour_of_day)) %>% 
  spread(key = order_dow, value = mean_time ) %>% 
  rename(Saturday = "0",
         Sunday = "1",
         Monday = "2",
         Tuesday = "3",
         Wednesday = "4",
         Thursday = "5",
         Friday = "6")

kable(mean_hour)
 
  
```

